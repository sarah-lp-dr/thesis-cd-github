{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a156ad0a",
   "metadata": {},
   "source": [
    "**Last Updated**: *29 May 2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f392ec",
   "metadata": {},
   "source": [
    "**Application 2D CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Modules ===\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn # nn models\n",
    "import torch.nn.functional as F # activation functions (incl. ReLu)\n",
    "from skimage.segmentation import flood\n",
    "from tqdm import tqdm\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import csv\n",
    "from skimage.measure import label, regionprops\n",
    "from scipy.ndimage import binary_erosion\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e93d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Functions ===\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Input: DEM (raster) file path \n",
    "    Output: DEM (array) and geotransform information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        gdal.UseExceptions()\n",
    "        data = gdal.Open(file_path)\n",
    "        if data is None:\n",
    "            raise FileNotFoundError(f\"Could not open file: {file_path}\")\n",
    "        data_array = data.ReadAsArray()\n",
    "        geotransform = data.GetGeoTransform()\n",
    "        return data_array, geotransform\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading DEM: {e}\")\n",
    "\n",
    "def normalize_data(data, min, max):\n",
    "    \"\"\"\n",
    "    Input: DEM (array)\n",
    "    Output: Normalized DEM (array)\n",
    "    \"\"\"\n",
    "    range = max - min\n",
    "    return (data - min) / range   # 0-1 range\n",
    "\n",
    "def detect_local_minima(dem_norm, elevation_range, window_size=5):\n",
    "    \"\"\"\n",
    "    Detects local minima based on elevation and slope criteria.\n",
    "    Returns a refined mask based on centrality and neighboring structure.\n",
    "    \"\"\"\n",
    "    rows, cols = dem_norm.shape\n",
    "    shape = (rows - window_size + 1, cols - window_size + 1, window_size, window_size)\n",
    "    strides = (dem_norm.strides[0], dem_norm.strides[1],\n",
    "               dem_norm.strides[0], dem_norm.strides[1])\n",
    "\n",
    "    dem_sliding_windows = as_strided(dem_norm, shape, strides)\n",
    "    local_minima = np.percentile(dem_sliding_windows, 40, axis=(2, 3)) \n",
    "    \n",
    "\n",
    "    center_values = dem_norm[window_size // 2:rows - window_size // 2,\n",
    "                             window_size // 2:cols - window_size // 2]\n",
    "\n",
    "    depth = np.ptp(dem_sliding_windows, axis=(2, 3))\n",
    "\n",
    "    mask = (center_values <= local_minima) & (depth >= 1.0 / elevation_range)   # crit1 & crit2\n",
    "\n",
    "    labeled_mask = label(mask.astype(bool), connectivity=1)\n",
    "    central_mask = np.zeros_like(mask, dtype=bool)\n",
    "\n",
    "    for region in regionprops(labeled_mask):\n",
    "        area = region.area\n",
    "        coords = region.coords\n",
    "        \n",
    "        if area < 2:    \n",
    "            continue    # crit3\n",
    "        \n",
    "        minr, minc, maxr, maxc = region.bbox\n",
    "        submask = np.zeros((maxr - minr, maxc - minc), dtype=bool)\n",
    "        submask[coords[:, 0] - minr, coords[:, 1] - minc] = True\n",
    "\n",
    "        if area > 30:\n",
    "            structure = np.array([[0, 1, 0],\n",
    "                                  [1, 1, 1],\n",
    "                                  [0, 1, 0]], dtype=bool)\n",
    "            eroded = binary_erosion(submask, structure=structure)\n",
    "            \n",
    "            if np.any(eroded):\n",
    "                submask = eroded\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        for subregion in regionprops(label(submask)):\n",
    "            centroid = subregion.centroid\n",
    "            r, c = int(round(centroid[0] + minr)), int(round(centroid[1] + minc))\n",
    "\n",
    "            if 0 <= r < labeled_mask.shape[0] and 0 <= c < labeled_mask.shape[1]:\n",
    "                central_mask[r, c] = True\n",
    "\n",
    "    return central_mask\n",
    "\n",
    "def extract_coordinates(mask, geotransform, window_size):\n",
    "    \"\"\"\n",
    "    Input: Local minima (mask), geotransform information and focal window size\n",
    "    Output: Local minima (coordinates)\n",
    "    \"\"\"\n",
    "    coordinates = []\n",
    "    for i, j in zip(*np.nonzero(mask)):\n",
    "        center_x = window_size // 2 + j\n",
    "        center_y = window_size // 2 + i\n",
    "        geo_x = geotransform[0] + center_x * geotransform[1] + center_y * geotransform[2]\n",
    "        geo_y = geotransform[3] + center_x * geotransform[4] + center_y * geotransform[5]\n",
    "        coordinates.append((len(coordinates) + 1, geo_x, geo_y))\n",
    "    return coordinates\n",
    "\n",
    "def save_to_csv(data, filename, headers):\n",
    "    \"\"\"\n",
    "    Input: Local minima (coordinates), filename and headers\n",
    "    Output: Local minima (CSV)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(headers)\n",
    "            writer.writerows(data)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error writing to CSV: {e}\")\n",
    "\n",
    "def extract_indices(depression_df, geotransform):\n",
    "    \"\"\"\n",
    "    Input: Depressions (dataframe) and geotransform information\n",
    "    Output: Depressions (dataframe) with location in pixel indices\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract geotransform parameters\n",
    "    x_origin = geotransform[0]\n",
    "    y_origin = geotransform[3]\n",
    "    pixel_width = geotransform[1]\n",
    "    pixel_height = geotransform[5]\n",
    "\n",
    "    # Convert to raster pixel indices\n",
    "    depression_df[\"col\"] = ((depression_df[\"longitude\"] - x_origin) / pixel_width).astype(int)\n",
    "    depression_df[\"row\"] = ((depression_df[\"latitude\"] - y_origin) / pixel_height).astype(int)\n",
    "\n",
    "    return depression_df\n",
    "\n",
    "# CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "       \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Global average pooling layer\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256*1*1, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, num_classes)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(0.4) # at least 0.4 for robustness\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU activation & batch normalization \n",
    "        # & Max pooling layers\n",
    "        x = F.relu(self.bn1(self.conv1(x)))         # (batch, 32, 50, 50)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))         # (batch, 64, 50, 50)\n",
    "        x = self.pool(x)                            # (batch, 64, 25, 25)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))         # (batch, 128, 25, 25)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))         # (batch, 256, 25, 25)\n",
    "        x = self.pool(x)                            # (batch, 256, 12, 12)\n",
    "\n",
    "        # Average pooling layer\n",
    "        x = self.global_pool(x)                     # (batch, 256, 1, 1)\n",
    "        \n",
    "        # Flatten\n",
    "        x = torch.flatten(x, 1)                     # (batch, 256)\n",
    "\n",
    "        # Fully connected layers with ReLU activation \n",
    "        # & Dropout layers\n",
    "        x = self.dropout(F.relu(self.fc1(x)))      # (batch, 128)\n",
    "        x = self.dropout(F.relu(self.fc2(x)))      # (batch, 64)\n",
    "        x = self.output(x)                         # (batch, 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e36806ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device will run on cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Load the model ===\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device will run on {device}\")\n",
    "\n",
    "model = CNN().to(device)\n",
    "model.load_state_dict(torch.load(\"cnn_2d_final_run.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Loop over all files ===\n",
    "tile_ids = [f'{i}' for i in range(1, 46)]\n",
    "window_size_local_minima = 9\n",
    "window_size_model = 75\n",
    "\n",
    "# New functions\n",
    "def process_local_minima_tile(dem_path_5m, output_csv, window_size):\n",
    "    dem_array, geotransform = load_data(dem_path_5m)\n",
    "    dem_array = np.where(dem_array < -50, np.nan, dem_array)\n",
    "    dem_array = normalize_data(dem_array, -50, 233.860001)\n",
    "    if dem_array.shape[0] < 25000 and dem_array.shape[1] < 12500:\n",
    "        print(\"Processing all at once!\")\n",
    "        split = 1\n",
    "    else: \n",
    "        print(\"Processing in minitiles!\")\n",
    "        split = 2\n",
    "    chunk_size = dem_array.shape[0] // split\n",
    "    margin = window_size // 2\n",
    "    results = []\n",
    "\n",
    "    nrows, ncols = dem_array.shape\n",
    "    total_chunks = ((nrows + chunk_size - 1) // chunk_size) * ((ncols + chunk_size - 1) // chunk_size)\n",
    "    row_indices = range(0, nrows, chunk_size)\n",
    "    col_indices = range(0, ncols, chunk_size)\n",
    "\n",
    "    for i, j in tqdm(product(row_indices, col_indices), total=total_chunks, desc=\"Processing minitiles\", unit=\"minitile\"):\n",
    "        i_start = max(i - margin, 0)\n",
    "        i_end = min(i + chunk_size + margin, nrows)\n",
    "        j_start = max(j - margin, 0)\n",
    "        j_end = min(j + chunk_size + margin, ncols)\n",
    "        if i_end <= i_start or j_end <= j_start:\n",
    "            continue\n",
    "        dem_chunk = dem_array[i_start:i_end, j_start:j_end]\n",
    "        if np.all(np.isnan(dem_chunk)):\n",
    "            continue\n",
    "        elevation_range = 283.860001\n",
    "        mask = detect_local_minima(dem_chunk, elevation_range, window_size)       \n",
    "        offset_transform = (\n",
    "            geotransform[0] + j_start * geotransform[1],\n",
    "            geotransform[1], 0,\n",
    "            geotransform[3] + i_start * geotransform[5],\n",
    "            0, geotransform[5]\n",
    "        )\n",
    "        coords = extract_coordinates(mask, offset_transform, window_size)\n",
    "        results.extend(coords)\n",
    "\n",
    "    unique_results = list({(c[1], c[2]): c for c in results}.values())\n",
    "    save_to_csv(unique_results, output_csv, ['id', 'longitude', 'latitude'])\n",
    "\n",
    "def run_model_on_minima(dem_path_1m, slope_path_1m, local_min_csv_path, output_csv, model, window_size):\n",
    "    dem_array, dem_transform = load_data(dem_path_1m)\n",
    "    dem_array = np.where(dem_array < -50, np.nan, dem_array)\n",
    "    dem_array = normalize_data(dem_array, -50, 233.860001)\n",
    "    slope_array, _ = load_data(slope_path_1m)\n",
    "    slope_array = np.where(slope_array < 0, np.nan, slope_array)\n",
    "    slope_array = normalize_data(slope_array, 0, 87.252808)\n",
    "    \n",
    "    local_min_df = pd.read_csv(local_min_csv_path, dtype={1: int, 2: int})\n",
    "    local_min_df = extract_indices(local_min_df, dem_transform)\n",
    "    \n",
    "    all_probs = []\n",
    "    half_window = window_size // 2\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(local_min_df.iterrows(), total=len(local_min_df)):\n",
    "            row_idx, col_idx = row[\"row\"], row[\"col\"]\n",
    "            row_min, row_max = max(row_idx - half_window, 0), min(row_idx + half_window, dem_array.shape[0])\n",
    "            col_min, col_max = max(col_idx - half_window, 0), min(col_idx + half_window, dem_array.shape[1])\n",
    "            \n",
    "            window_dem = dem_array[row_min:row_max, col_min:col_max]\n",
    "            window_slope = slope_array[row_min:row_max, col_min:col_max]\n",
    "\n",
    "            seed_row, seed_col = row_idx - row_min, col_idx - col_min\n",
    "            if (0 <= seed_row < window_dem.shape[0]) and (0 <= seed_col < window_dem.shape[1]):\n",
    "                window_flood = flood(window_dem, (seed_row, seed_col), tolerance=0.005)\n",
    "            else:\n",
    "                window_flood = np.zeros_like(window_dem, dtype=np.float32)\n",
    "\n",
    "            feature_stack = np.stack([\n",
    "                window_dem,\n",
    "                window_slope,\n",
    "                window_flood\n",
    "            ])\n",
    "\n",
    "            input_tensor = torch.from_numpy(feature_stack).unsqueeze(0).float().to(device)\n",
    "\n",
    "            output = model(input_tensor).squeeze(0)\n",
    "            prob = torch.sigmoid(output).item()\n",
    "\n",
    "            all_probs.append(prob)\n",
    "            \n",
    "    local_min_df[\"prob\"] = all_probs\n",
    "    local_min_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Results saved to {output_csv}\")\n",
    "\n",
    "# Main loop\n",
    "for tile_id in tile_ids:\n",
    "    print(f\"\\n=== Processing tile {tile_id} ===\")\n",
    "    \n",
    "    # Step 1 — local minima on 5m DEM\n",
    "    dem_5m_path = f\"D:/all_tifs/dem_{tile_id}_5m.tif\"\n",
    "    local_minima_csv = f\"D:/all_results/local_minima_{tile_id}.csv\"\n",
    "    process_local_minima_tile(dem_5m_path, local_minima_csv, window_size_local_minima)\n",
    "    \n",
    "    # Step 2 — model on 1m DEM + slope\n",
    "    dem_1m_path = f\"D:/all_tifs/dem_{tile_id}_1m.tif\"\n",
    "    slope_1m_path = f\"D:/all_tifs/slope_{tile_id}_1m.tif\"\n",
    "    output_csv = f\"D:/all_results/updated_local_minima_{tile_id}.csv\"\n",
    "    run_model_on_minima(dem_1m_path, slope_1m_path, local_minima_csv, output_csv, model, window_size_model)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_remote_sensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
